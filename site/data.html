<!DOCTYPE html>
<html lang="en">
    <head>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="stylesheet" href="../links/main.css" type="text/css">
<link rel="alternate" type="application/rss+xml" title="Subscribe to this page..." href="rss.xml">
<meta name="author" content="Tunjan">
<meta property="og:image" content="https://tunjan.github.io/media/logo.png">
<link rel="shortcut icon" type="image/png" href="/media/logo.png">
<link rel="icon" type="image/png" href="/media/logo.png"/>
<meta charset="utf-8">
<title>Tunjan ‒ data</title>
   
    </head>
    <body>
      <header>
        <a href="home.html">
  <img src="../media/logo.png" width ="80px">
</a>


















      </header>
     <nav><ul><li>
          <a href="about.html"> about</a>
        </li><li>
          <a href="tools.html"> tools</a>
        </li><li>
          <a href="books.html"> books</a>
        </li><li>
          <a href="research.html"> research</a>
        </li><li>
          <a href="sites.html"> sites</a>
        </li><li>
          <a href="commands.html"> commands</a>
        </li><li>
          <a id="selected" href="data.html">data</a>
        </li></ul></nav>

     <main>

    <hr />

     <div id="ToC"></div>
     
        <h2>Data analysis</h2>
<h3>Preprocessing</h3>
<pre><code class="language-python">X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

num_features = [feature for feature in X_train.columns if X_train[feature].dtype in [&#39;int64&#39;, &#39;float64&#39;]]
cat_features = [feature for feature in X_train.columns if X_train[feature].nunique() &lt; 10 and X_train[feature].dtype == &quot;object&quot;]

my_cols = cat_features + num_features

X_train = X_train[my_cols].copy()
X_val = X_val[my_cols].copy()
X_test = test_df[my_cols].copy()

cat_transformer = Pipeline([
    (&#39;imputer&#39;, SimpleImputer(strategy=&#39;most_frequent&#39;)),
    (&#39;encoder&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;))
])

num_transformer = Pipeline([
    (&#39;imputer&#39;, SimpleImputer(strategy=&#39;median&#39;)),
    (&#39;scaler&#39;, StandardScaler())
])

preprocessor = ColumnTransformer([
    (&#39;num&#39;, num_transformer, num_features),
    (&#39;cat&#39;, cat_transformer, cat_features)
])

pipeline = Pipeline([
    (&#39;preprocessor&#39;, preprocessor),
    (&#39;classifier&#39;, RandomForestClassifier())
])
</code></pre>
<h3>Hyperparameter tunning</h3>
<pre><code class="language-python">xgb_model = XGBClassifier(
    learning_rate=0.01, 
    n_estimators=400, 
    objective=&#39;binary:logistic&#39;,
    nthread=1
)

pipeline_xgb = Pipeline(steps=[
    (&#39;preprocessor&#39;, preprocessor),
    (&#39;classifier&#39;, xgb_model)
])

params_xgb = {
    &#39;classifier__min_child_weight&#39;: [1, 5, 7, 10],
    &#39;classifier__gamma&#39;: [0.2, 0.5, 1, 1.5, 2, 5],
    &#39;classifier__subsample&#39;: [0.4, 0.6, 0.8, 1.0],
    &#39;classifier__colsample_bytree&#39;: [0.4, 0.6, 0.8, 1.0],
    &#39;classifier__max_depth&#39;: [2, 3, 4, 5, 6, 7],
    &#39;classifier__learning_rate&#39;: [0.001, 0.01, 0.1],
    &#39;classifier__n_estimators&#39;: [100, 200, 400, 600]
}

folds = 20
param_comb = 20
skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)
random_search = RandomizedSearchCV(pipeline_xgb, param_distributions=params_xgb, n_iter=param_comb, scoring=&#39;accuracy&#39;,n_jobs=4, cv=skf.split(X_train,y_train), verbose=3, random_state=1001 )
random_search.fit(X_train, y_train)

pipeline_xgb.set_params(**random_search.best_params_)
pipeline_xgb.fit(X_train, y_train)

y_val_xgb_pred = pipeline_xgb.predict(X_val)
accuracy = classification_report(y_val, y_val_xgb_pred)
print(f&quot;Accuracy: {accuracy}&quot;)

pipeline_xgb.fit(X, y)

y_pred_test_xgb = pipeline_xgb.predict(X_test)
</code></pre>
<h3>Model Selection</h3>
<pre><code class="language-python">
classifiers = {
    &quot;LogisticRegression&quot; : LogisticRegression(random_state=0),
    &quot;KNN&quot; : KNeighborsClassifier(),
    &quot;SVC&quot; : SVC(random_state=0, probability=True),
    &quot;RandomForest&quot; : RandomForestClassifier(random_state=0),
    &quot;XGBoost&quot; : XGBClassifier(random_state=0, use_label_encoder=False, eval_metric=&#39;logloss&#39;), # XGBoost takes too long
    &quot;LGBM&quot; : LGBMClassifier(random_state=0),
    &quot;CatBoost&quot; : CatBoostClassifier(random_state=0, verbose=False),
    &quot;NaiveBayes&quot;: GaussianNB()
}

LR_grid = {&#39;penalty&#39;: [&#39;l1&#39;,&#39;l2&#39;],
           &#39;C&#39;: [0.25, 0.5, 0.75, 1, 1.25, 1.5],
           &#39;max_iter&#39;: [50, 100, 150]}

KNN_grid = {&#39;n_neighbors&#39;: [3, 5, 7, 9],
            &#39;p&#39;: [1, 2]}

SVC_grid = {&#39;C&#39;: [0.25, 0.5, 0.75, 1, 1.25, 1.5],
            &#39;kernel&#39;: [&#39;linear&#39;, &#39;rbf&#39;],
            &#39;gamma&#39;: [&#39;scale&#39;, &#39;auto&#39;]}

RF_grid = {&#39;n_estimators&#39;: [50, 100, 150, 200, 250, 300],
        &#39;max_depth&#39;: [4, 6, 8, 10, 12]}

boosted_grid = {&#39;n_estimators&#39;: [50, 100, 150, 200],
        &#39;max_depth&#39;: [4, 8, 12],
        &#39;learning_rate&#39;: [0.05, 0.1, 0.15]}

NB_grid={&#39;var_smoothing&#39;: [1e-10, 1e-9, 1e-8, 1e-7]}

# Dictionary of all grids
grid = {
    &quot;LogisticRegression&quot; : LR_grid,
    &quot;KNN&quot; : KNN_grid,
    &quot;SVC&quot; : SVC_grid,
    &quot;RandomForest&quot; : RF_grid,
    &quot;XGBoost&quot; : boosted_grid,
    &quot;LGBM&quot; : boosted_grid,
    &quot;CatBoost&quot; : boosted_grid,
    &quot;NaiveBayes&quot;: NB_grid
}


i=0
clf_best_params=classifiers.copy()
valid_scores=pd.DataFrame({&#39;Classifer&#39;:classifiers.keys(), &#39;Validation accuracy&#39;: np.zeros(len(classifiers)), &#39;Training time&#39;: np.zeros(len(classifiers))})
for key, classifier in classifiers.items():
    start = time.time()
    clf = GridSearchCV(estimator=classifier, param_grid=grid[key], n_jobs=-1, cv=None)

    # Train and score
    clf.fit(X_train, y_train)
    valid_scores.iloc[i,1]=clf.score(X_valid, y_valid)

    # Save trained model
    clf_best_params[key]=clf.best_params_
    
    # Print iteration and training time
    stop = time.time()
    valid_scores.iloc[i,2]=np.round((stop - start)/60, 2)
    
    print(&#39;Model:&#39;, key)
    print(&#39;Training time (mins):&#39;, valid_scores.iloc[i,2])
    print(&#39;&#39;)
    i+=1

valid_scores

</code></pre>

      

      </main>
      <footer><hr />
<span style='float:right'><a href='https://codeberg.org/tunjan/pages'> [Codeberg]</a>
<br/>
</span> 
<a rel="me" href="https://scicomm.xyz/@tunjan">Tunjan</a> © 2023 
</footer>
    <script src="../links/scripts/toc.js"></script>
    </body>
</html>
